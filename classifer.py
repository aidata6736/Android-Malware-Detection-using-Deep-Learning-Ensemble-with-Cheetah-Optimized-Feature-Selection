from keras.layers import Flatten
from sklearn.preprocessing import StandardScaler
from keras.layers import Bidirectional, LSTM
from keras.layers import SimpleRNN
from tensorflow.keras.layers import Dense, Flatten, Concatenate, Input, Attention
from keras.models import Sequential
from keras.layers import Dense, Dropout
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from confusion_met import *
from keras.optimizers import Adam
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Dropout, Bidirectional, SimpleRNN, GRU
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LeakyReLU
from tensorflow.keras.optimizers import Adam
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

def bi_lstm(x_train, y_train, x_test, y_test):
    # Standardize the input features
    scaler = StandardScaler()
    x_train = scaler.fit_transform(x_train)
    x_test = scaler.transform(x_test)

    # Reshape data for LSTM input (assuming x_train and x_test are 2D arrays)
    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)

    # Create a Sequential model
    model = Sequential()

    # Bidirectional LSTM layer
    model.add(Bidirectional(LSTM(64, activation='relu', return_sequences=True), input_shape=(x_train.shape[1], 1)))
    model.add(Bidirectional(LSTM(32, activation='relu')))
    model.add(Dense(5, activation='softmax'))
    # Compile the model
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    # Train the model
    model.fit(x_train, y_train, epochs=1, batch_size=32, validation_split=0.2, verbose=1,steps_per_epoch=10)
    # Make predictions on the test data
    y_pred = np.argmax(model.predict(x_test), axis=1)
    cm = multi_confu_matrix(y_test, y_pred)
    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    return y_pred, cm

from keras.utils import to_categorical


def deep_fnn(X_train, y_train, X_test, y_test):
    # One-hot encode the labels
    y_train_encoded = to_categorical(y_train, num_classes=4)
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Define the model
    model = Sequential()

    # Input layer
    model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))

    # Additional hidden layers
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.2))  # Dropout layer for regularization
    model.add(Dense(32, activation='relu'))

    # Output layer
    model.add(Dense(4, activation='softmax'))

    # Compile the model
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

    # Print model summary
    model.summary()

    # Train the model
    model.fit(X_train, y_train_encoded, epochs=1, batch_size=16, validation_split=0.2,steps_per_epoch=10)

    # Predict on test data
    y_pred_encoded = model.predict(X_test)
    y_pred = np.argmax(y_pred_encoded, axis=1)

    return y_pred, multi_confu_matrix(y_pred, y_test)


def deep_RNN(x_train, y_train, x_test, y_test):
    # Standardize the input features
    scaler = StandardScaler()
    x_train = scaler.fit_transform(x_train)
    x_test = scaler.transform(x_test)

    # Reshape data for RNN input (assuming x_train and x_test are 2D arrays)
    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)

    # Create a Sequential model
    model = Sequential()
    model.add(SimpleRNN(64, activation='relu', input_shape=(x_train.shape[1], 1)))

    # Output layer
    model.add(Dense(4, activation='softmax'))

    # Compile the model
    optimizer = Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    # Print model summary
    model.summary()

    # Convert labels to one-hot encoding
    y_train_one_hot = to_categorical(y_train, 4)
    y_test_one_hot = to_categorical(y_test, 4)

    # Train the model
    model.fit(x_train, y_train_one_hot, epochs=1, batch_size=16, validation_split=0.2)

    # Predict on test data
    y_pred_one_hot = model.predict(x_test)

    # Convert predictions back to labels
    y_pred = np.argmax(y_pred_one_hot, axis=1)

    return y_pred, confu_matrix(y_pred, y_test)


def Gru(X_train, Y_train, X_test, Y_test):
    # Convert DataFrame to NumPy array
    X_train = X_train.values
    X_test = X_test.values

    # Reshape data
    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

    model = Sequential()
    model.add(GRU(64, input_shape=(X_train.shape[1], 1)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(4, activation='softmax'))

    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.fit(X_train, Y_train, epochs=2, batch_size=50, verbose=0)

    y_predict = np.argmax(model.predict(X_test), axis=1)
    conf_matrix = multi_confu_matrix(Y_test, y_predict)

    return y_predict, conf_matrix




def build_generator(latent_dim, output_dim):
    model = Sequential()
    model.add(Dense(32, input_dim=latent_dim, activation='relu'))
    model.add(Dense(output_dim, activation='linear'))
    return model

def build_discriminator(input_dim):
    model = Sequential()
    model.add(Dense(32, input_dim=input_dim, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    return model

def build_gan(generator, discriminator):
    discriminator.trainable = False
    model = Sequential()
    model.add(generator)
    model.add(discriminator)
    return model

def generate_real_samples(n_samples):
    X = np.random.rand(n_samples) * 2 - 1
    y = np.ones((n_samples, 1))
    return X, y

def generate_fake_samples(generator, latent_dim, n_samples):
    noise = np.random.randn(latent_dim * n_samples).reshape(n_samples, latent_dim)
    X = generator.predict(noise)
    y = np.zeros((n_samples, 1))
    return X, y

def train_gan(generator, discriminator, gan_model, latent_dim, n_epochs=10000, n_batch=64):
    for epoch in range(n_epochs):
        X_real, y_real = generate_real_samples(n_batch // 2)
        X_fake, y_fake = generate_fake_samples(generator, latent_dim, n_batch // 2)

        d_loss_real = discriminator.train_on_batch(X_real, y_real)
        d_loss_fake = discriminator.train_on_batch(X_fake, y_fake)

        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        noise = np.random.randn(latent_dim * n_batch).reshape(n_batch, latent_dim)
        y_gen = np.ones((n_batch, 1))
        g_loss = gan_model.train_on_batch(noise, y_gen)

        if epoch % 100 == 0:
            print(f"{epoch} [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss}]")

def GAN(X_train, Y_train, X_test, Y_test):
    latent_dim = 10
    output_dim = 1
    n_epochs = 100
    n_batch = 64
    discriminator = build_discriminator(output_dim)
    discriminator.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy',
                          metrics=['accuracy'])

    generator = build_generator(latent_dim, output_dim)

    gan_model = build_gan(generator, discriminator)
    gan_model.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy')

    train_gan(generator, discriminator, gan_model, latent_dim, n_epochs, n_batch)

    # Convert X_test to a NumPy array and reshape it to match the discriminator's input shape
    X_test_reshaped = X_test.values.reshape(-1, output_dim)

    # Predict labels for the real and generated samples
    y_real_pred = discriminator.predict(X_test_reshaped)

    noise = np.random.randn(X_test.shape[0] * latent_dim).reshape(X_test.shape[0], latent_dim)
    generated_samples = generator.predict(noise)

    # Reshape generated samples to match the discriminator's input shape
    generated_samples_reshaped = generated_samples.reshape(-1, output_dim)

    y_generated_pred = discriminator.predict(generated_samples_reshaped)

    # Evaluate accuracy
    accuracy_real = np.mean((y_real_pred > 0.5).astype(int) == Y_test)
    accuracy_generated = np.mean((y_generated_pred <= 0.5).astype(int) == 0)

    return accuracy_real, accuracy_generated



def KNN(X_train, y_train, X_test, y_test):
    # Create a KNN classifier with k=3
    knn_classifier = KNeighborsClassifier(n_neighbors=3)

    # Train the classifier
    knn_classifier.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = knn_classifier.predict(X_test)

    return y_pred, multi_confu_matrix(y_test, y_pred)



def RF(X_train, Y_train, X_test, Y_test):
    model = RandomForestRegressor(max_depth=2, random_state=0)
    model.fit(X_train, Y_train)

    # Predict on the test set
    y_predict = model.predict(X_test)

    # Evaluate the model
    mse = mean_squared_error(Y_test, y_predict)
    print("Mean Squared Error:", mse)

    return y_predict, multi_confu_matrix(Y_test, y_predict)

def SVM():
    # Generate synthetic data
    X, y = make_classification(n_samples=28450, n_features=13, n_classes=2, random_state=42)

    # Split the data into training and testing sets
    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Create SVM model
    svm_model = SVC(kernel='linear')

    # Train the model
    svm_model.fit(X_train, Y_train)

    # Make predictions on the test set
    y_pred = svm_model.predict(X_test)

    # Get the confusion matrix
    conf_matrix = confusion_matrix(Y_test, y_pred)

    return y_pred, conf_matrix




def knn_svm_rf():
    # Generate synthetic data
    X, y = make_classification(n_samples=28450, n_features=13, n_classes=2, random_state=42)

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train KNN
    knn_classifier = KNeighborsClassifier(n_neighbors=3)
    knn_classifier.fit(X_train, y_train)

    # Train Random Forest
    rf_classifier = RandomForestClassifier(max_depth=2, random_state=0)
    rf_classifier.fit(X_train, y_train)

    # Train SVM
    svm_classifier = SVC()
    svm_classifier.fit(X_train, y_train)

    # Create an ensemble model
    ensemble_model = VotingClassifier(estimators=[
        ('knn', knn_classifier),
        ('rf', rf_classifier),
        ('svm', svm_classifier)
    ], voting='hard')  # Use 'hard' voting for classification

    # Train the ensemble model
    ensemble_model.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = ensemble_model.predict(X_test)

    # Evaluate the accuracy
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy:.2f}")

    # Print classification report
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    return y_pred,multi_confu_matrix(y_test, y_pred)
def EnsembleNet(X_train, y_train, X_test, y_test):
    lstm_output, _ = bi_lstm(X_train, y_train, X_test, y_test)
    deep_rnn_output, _ = deep_RNN(X_train, y_train, X_test, y_test)
    deep_fnn_output, _ = deep_fnn(X_train, y_train, X_test, y_test)

    # Flatten the outputs before passing them to Dense layers
    lstm_output = Flatten()(lstm_output)
    deep_rnn_output = Flatten()(deep_rnn_output)
    deep_fnn_output = Flatten()(deep_fnn_output)

    # Assuming the output shapes are (batch_size, num_features) for each model
    lstm_output = Dense(6, activation='relu')(lstm_output)
    deep_rnn_output = Dense(6, activation='relu')(deep_rnn_output)
    deep_fnn_output = Dense(6, activation='relu')(deep_fnn_output)

    # Cast the int32 tensor to float32
    lstm_output = tf.cast(lstm_output, tf.float32)

    # Concatenate the outputs of the three models
    concatenated_output = Concatenate()([lstm_output, deep_rnn_output, deep_fnn_output])

    # Create the model
    model = Sequential()

    # Compile the model for regression
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # Train the model
    model.fit([X_train, X_train, X_train], y_train, epochs=1, batch_size=8, verbose=1)

    # Evaluate the model
    y_predict = model.predict([X_test, X_test, X_test])

    # Choose the output head you want to use for evaluation
    output_index = 0  # Replace with the correct index based on your model architecture

    # Extract the predictions for the chosen output head
    y_pred = np.argmax(y_predict[output_index], axis=1)
    print(y_pred)

    return y_predict,multi_confu_matrix(y_test, y_pred)

