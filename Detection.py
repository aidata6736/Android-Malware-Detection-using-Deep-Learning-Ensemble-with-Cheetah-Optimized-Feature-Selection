'''from tensorflow.keras.layers import Conv2D, MaxPooling2D, concatenate, Input, AveragePooling2D, Flatten, Dense, BatchNormalization, Activation, Concatenate
from tensorflow.keras.models import Model
import numpy as np
from Confusion_matrix import multi_confu_matrix


def normal_cell(input_tensor):
    # First Branch
    branch_1 = Conv2D(32, (1, 1), padding='same', activation='relu')(input_tensor)

    # Second Branch
    branch_2 = Conv2D(32, (1, 1), padding='same', activation='relu')(input_tensor)
    branch_2 = Conv2D(32, (3, 3), padding='same', activation='relu')(branch_2)

    # Third Branch
    branch_3 = Conv2D(32, (1, 1), padding='same', activation='relu')(input_tensor)
    branch_3 = Conv2D(32, (5, 5), padding='same', activation='relu')(branch_3)

    # Fourth Branch
    branch_4 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_tensor)
    branch_4 = Conv2D(32, (1, 1), padding='same', activation='relu')(branch_4)

    # Concatenate Branches
    output_tensor = concatenate([branch_1, branch_2, branch_3, branch_4], axis=-1)
    return output_tensor


def reduction_cell(input_tensor):
    # First Branch
    branch_1 = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(input_tensor)

    # Second Branch
    branch_2 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_tensor)
    branch_2 = Conv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(branch_2)

    # Third Branch
    branch_3 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_tensor)
    branch_3 = Conv2D(64, (5, 5), strides=(2, 2), padding='same', activation='relu')(branch_3)

    # Fourth Branch
    branch_4 = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(input_tensor)

    # Concatenate Branches
    output_tensor = concatenate([branch_1, branch_2, branch_3, branch_4], axis=-1)
    return output_tensor


def conv_block(x, growth_rate):
    # Bottleneck layer
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(4 * growth_rate, (1, 1), padding='same')(x)

    # Convolution layer
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(growth_rate, (1, 1), padding='same')(x)

    return x


def dense_block(x, num_layers, growth_rate):
    for _ in range(num_layers):
        conv = conv_block(x, growth_rate)
        x = Concatenate(axis=-1)([x, conv])
    return x

def transition_block(x):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = AveragePooling2D((1, 1), strides=(2, 2))(x)
    return x

def NASNet(x_train, y_train, x_test, y_test,  num_dense_blocks=3, growth_rate=32, num_layers_per_block=4):
    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1, 1)
    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1, 1)
    num_classes = len(set(y_test))

    input_shape = x_train[1].shape

    input_tensor = Input(shape=input_shape)

    # Initial Convolution
    x = Conv2D(64, (3, 3), padding='same', activation='relu')(input_tensor)
    # Normal Cell 1
    x = normal_cell(x)
    # Reduction Cell 1
    x = reduction_cell(x)
    # Normal Cell 2
    x = normal_cell(x)

    for i in range(num_dense_blocks):
        x = dense_block(x, num_layers_per_block, growth_rate)
        if i != num_dense_blocks - 1:
            x = transition_block(x)

    # Global Average Pooling
    x = AveragePooling2D((1, 1))(x)


    # Flatten
    x = Flatten()(x)
    # Fully Connected Layer
    x = Dense(10, activation='relu')(x)
    # Output Layer
    output_tensor = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=input_tensor, outputs=output_tensor)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    model.fit(x_train, y_train, epochs=200, verbose=1)
    pred = np.argmax(model.predict(x_test), axis=1)
    met = multi_confu_matrix(y_test, pred)
    return met


from save_load import load

x_train = load('x_train_60')
y_train = load('y_train_60')
x_test = load('x_test_60')
y_test = load('y_test_60')
NASNet(x_train, y_train, x_test, y_test)'''

from tensorflow.keras.layers import Conv2D, MaxPooling2D, concatenate, Input, AveragePooling2D, Flatten, Dense, BatchNormalization, Activation, Concatenate, Lambda, Bidirectional, LSTM
from sklearn.neural_network import MLPClassifier
from tensorflow.keras.models import Model, Sequential
import numpy as np
from sklearn.svm import SVC
from Confusion_matrix import multi_confu_matrix

def normal_cell(input_tensor):
    # First Branch
    branch_1 = Conv2D(32, (1, 1), padding='same', activation='relu')(input_tensor)

    # Second Branch
    branch_2 = Conv2D(32, (1, 1), padding='same', activation='relu')(input_tensor)
    branch_2 = Conv2D(32, (3, 3), padding='same', activation='relu')(branch_2)

    # Third Branch
    branch_3 = Conv2D(32, (1, 1), padding='same', activation='relu')(input_tensor)
    branch_3 = Conv2D(32, (5, 5), padding='same', activation='relu')(branch_3)

    # Fourth Branch
    branch_4 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_tensor)
    branch_4 = Conv2D(32, (1, 1), padding='same', activation='relu')(branch_4)

    # Concatenate Branches
    output_tensor = concatenate([branch_1, branch_2, branch_3, branch_4], axis=-1)
    return output_tensor

def reduction_cell(input_tensor):
    # First Branch
    branch_1 = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(input_tensor)

    # Second Branch
    branch_2 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_tensor)
    branch_2 = Conv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(branch_2)

    # Third Branch
    branch_3 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_tensor)
    branch_3 = Conv2D(64, (5, 5), strides=(2, 2), padding='same', activation='relu')(branch_3)

    # Fourth Branch
    branch_4 = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(input_tensor)

    # Concatenate Branches
    output_tensor = concatenate([branch_1, branch_2, branch_3, branch_4], axis=-1)
    return output_tensor

def conv_block(x, growth_rate):
    # Bottleneck layer
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(4 * growth_rate, (1, 1), padding='same')(x)

    # Convolution layer
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(growth_rate, (1, 1), padding='same')(x)

    return x

def dense_block(x, num_layers, growth_rate):
    for _ in range(num_layers):
        conv = conv_block(x, growth_rate)
        x = Concatenate(axis=-1)([x, conv])
    return x

def transition_block(x):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = AveragePooling2D((1, 1), strides=(2, 2))(x)
    return x

def create_base_network(input_shape):
    input_tensor = Input(shape=input_shape)

    # Initial Convolution
    x = Conv2D(64, (3, 3), padding='same', activation='relu')(input_tensor)
    # Normal Cell 1
    x = normal_cell(x)
    # Reduction Cell 1
    x = reduction_cell(x)
    # Normal Cell 2
    x = normal_cell(x)

    # Dense Blocks
    for i in range(3):
        x = dense_block(x, 4, 32)
        if i != 2:
            x = transition_block(x)

    # Global Average Pooling
    x = AveragePooling2D((1, 1))(x)
    x = Flatten()(x)

    # Fully Connected Layer
    x = Dense(256, activation='relu')(x)
    model = Model(inputs=input_tensor, outputs=x)
    return model

def create_siamese_network(input_shape, num_classes):
    base_network = create_base_network(input_shape)   # NASNet + DenseNet
    input_a = Input(shape=input_shape)
    input_b = Input(shape=input_shape)

    processed_a = base_network(input_a)
    processed_b = base_network(input_b)

    distance = Lambda(lambda tensors: abs(tensors[0] - tensors[1]))([processed_a, processed_b])
    similarity = Dense(num_classes, activation='softmax')(distance)

    siamese_network = Model(inputs=[input_a, input_b], outputs=similarity)
    return siamese_network

def DSDNN(x_train, y_train, x_test, y_test):
    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1, 1)
    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1, 1)
    num_classes = len(set(y_test))
    input_shape = x_train[1].shape

    siamese_model = create_siamese_network(input_shape, num_classes)
    siamese_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    history = siamese_model.fit([x_train, x_train], y_train, epochs=200, batch_size=32, verbose=0, validation_data=([x_test, x_test], y_test))

    predictions = np.argmax(siamese_model.predict([x_test, x_test]), axis=1)
    met = multi_confu_matrix(y_test, predictions)
    return predictions, met, history



# Support vector machines
def SVM(x_train, y_train, x_test, y_test):
    # Train SVM classifier
    svm_classifier = SVC()
    svm_classifier.fit(x_train, y_train)

    predict = svm_classifier.predict(x_test)
    met = multi_confu_matrix(y_test, predict)
    return predict, met


# Bidirectional Long-Short Term Memory
def Bi_LSTM(x_train, y_train, x_test, y_test):
    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)
    num_classes = y_train.max() + 1

    model = Sequential()
    model.add(Bidirectional(LSTM(64, input_shape=x_train[1].shape, activation='relu')))
    model.add(Dense(num_classes, activation='softmax'))

    # train Bi_LSTM model
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.fit(x_train, y_train, epochs=100, batch_size=100)

    y_predict = np.argmax(model.predict(x_test), axis=1)
    return y_predict, multi_confu_matrix(y_test, y_predict)


def CNN(x_train, y_train, x_test, y_test):
    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1, 1)
    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1, 1)
    num_classes = y_train.max() + 1

    model = Sequential()

    model.add(Conv2D(32, (1, 1), activation='relu', padding='same', input_shape=x_train[1].shape))
    model.add(MaxPooling2D(pool_size=(1, 1)))
    model.add(Conv2D(64, (1, 1), activation='relu', padding='same'))

    model.add(Flatten())
    model.add(Dense(64, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    model.fit(x_train, y_train, epochs=100, batch_size=32)

    pred = np.argmax(model.predict(x_test), axis=1)

    return pred, multi_confu_matrix(y_test, pred)


def ANN(x_train, y_train, x_test, y_test):
    ann_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)
    # Train the classifier on the training data
    ann_classifier.fit(x_train, y_train)

    # Make predictions on the test data
    y_pred = ann_classifier.predict(x_test)
    met = multi_confu_matrix(y_test, y_pred)
    return y_pred, met



